{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0879c67-709c-42c8-920d-11e8d398e88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c16b87-cd99-45f1-8823-9c1494c32f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c61c5e-2aed-4cb5-b084-ae60da35c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f5a7678-24c2-44b3-96e2-70113c06bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cbc6568-d701-4ed0-a4e6-2c78039f519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import glob\n",
    "from PIL import Image\n",
    "from pytesseract import pytesseract\n",
    "from lxml import etree\n",
    "import ast\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2d1c2-a64d-42be-b4a5-661568e79f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265988b9-3b06-42de-874b-54f5d75ec382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c6269dc-1712-4577-a30c-b88610749441",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../assets/doc-0001.json')\n",
    "label_studio_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d8a95-ca8a-4ebd-8e3d-cb4a68a04549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ef12179-a907-4c88-8f08-16ff5ebea13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box_1, box_2):\n",
    "    poly_1 = Polygon(box_1)\n",
    "    poly_2 = Polygon(box_2)\n",
    "    # print(poly_1,poly_2)\n",
    "    # iou = poly_1.intersection(poly_2).area / poly_1.union(poly_2).area\n",
    "    iou = poly_1.intersection(poly_2).area\n",
    "    min_area = min(poly_1.area,poly_2.area)\n",
    "    return iou/min_area\n",
    "    \n",
    "    \n",
    "def hocr_to_dataframe(fp):\n",
    "    doc = etree.parse(fp)\n",
    "    words = []\n",
    "    wordConf = []\n",
    "    coords_list = []\n",
    "    for path in doc.xpath('//*'):\n",
    "        if 'ocrx_word' in path.values():\n",
    "            coord_text = path.values()[2].split(';')[0].split(' ')[1:] \n",
    "            word_coord = list(map(int, coord_text)) #x1, y1, x2, y2\n",
    "            conf = [x for x in path.values() if 'x_wconf' in x][0]\n",
    "            wordConf.append(int(conf.split('x_wconf ')[1]))\n",
    "            words.append(path.text)\n",
    "            coords_list.append(word_coord)\n",
    "\n",
    "    dfReturn = pd.DataFrame({'word' : words,\n",
    "                             'coords': coords_list,\n",
    "                             'confidence' : wordConf})\n",
    "\n",
    "    return(dfReturn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "51c0f122-ae15-4ea3-89c5-bde2af16d008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lxml.etree._ElementTree at 0x2600a2bc740>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etree.parse('./data/layoutlmv3_hocr_output/doc-0001.hocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d3aaf-e1bc-429d-9db9-746e79d7d5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "427da69b-fb48-4aa5-aa5d-5627595b369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_data = dict()\n",
    "document_data['file_name'] = []\n",
    "document_data['labelled_bbox']= []\n",
    "\n",
    "for i in range(len(label_studio_data)):\n",
    "    row = label_studio_data[i]\n",
    "    file_name = os.path.basename(row['data']['image'])\n",
    "    label_list, labels, bboxes = [], [], []\n",
    "\n",
    "    for label_ in row['annotations'][0]['result']:\n",
    "        label_value = label_['value']\n",
    "        x, y, w, h = label_value['x'], label_value['y'], label_value['width'], label_value['height']\n",
    "        original_w , original_h = label_['original_width'], label_['original_height']\n",
    "\n",
    "        x1 = int((x * original_w) / 100)\n",
    "        y1 = int((y * original_h) / 100)\n",
    "        x2 = x1 + int(original_w*w / 100)\n",
    "        y2 = y1 + int(original_h*h / 100)\n",
    "        \n",
    "        label = label_value['rectanglelabels']\n",
    "        label_list.append((label, (x1,y1,x2,y2), original_h, original_w))\n",
    "        \n",
    "    document_data['file_name'].append(file_name)    \n",
    "    document_data['labelled_bbox'].append(label_list)        \n",
    "\n",
    "custom_dataset = pd.DataFrame(document_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0422e1b7-bba5-425b-9895-0e058074e43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['doc_number'], (1926, 298, 2165, 419), 2491, 3489),\n",
       " (['doc_date'], (1145, 489, 2310, 580), 2491, 3489),\n",
       " (['supplier_name'], (774, 702, 1361, 895), 2491, 3489),\n",
       " (['supplier_phone'], (2349, 741, 2897, 813), 2491, 3489)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset['labelled_bbox'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a78c8ee0-3352-49df-9b66-75748ade5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label2id = {\"doc_number\": 0, \"doc_date\": 1, \"supplier_name\": 2, \"supplier_phone\": 3, \"receiver_tax_id\": 4 }\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15dabc7-e207-4726-b3ef-7cc866cdf9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f42360-72c5-483d-a75e-0f259870b1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ae4584f-42b8-43e7-ac5e-de5c1ed7fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:19<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 438 ms\n",
      "Wall time: 19.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "final_list = []\n",
    "    \n",
    "for i in tqdm(custom_dataset.iterrows(), total=custom_dataset.shape[0]):\n",
    "    custom_label_text = {}\n",
    "    word_list = []\n",
    "    ner_tags_list  = []\n",
    "    bboxes_list = []\n",
    "    \n",
    "    file_name = i[1]['file_name']\n",
    "    for image in glob.glob('../assets/doclabel/*.jpg'): \n",
    "        frame_file_name = os.path.basename(image)\n",
    "        if frame_file_name == file_name:\n",
    "            custom_label_text['id'] = i[0]\n",
    "            image_basename = os.path.basename(image)\n",
    "            custom_label_text['file_name'] = image_basename\n",
    "            annotations = []\n",
    "            label_coord_list = i[1]['labelled_bbox']\n",
    "            for label_coord in label_coord_list:\n",
    "                (x1,y1,x2,y2) = label_coord[1]\n",
    "                box1 = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]] \n",
    "                label = label_coord[0][0]\n",
    "                base_name = os.path.join('./data', 'layoutlmv3_hocr_output',os.path.basename(image).split('.')[0])\n",
    "                pytesseract.run_tesseract(image, base_name, extension='box', lang=None, config=\"hocr\")\n",
    "                hocr_file = os.path.join(base_name+'.hocr')\n",
    "                hocr_df = hocr_to_dataframe(hocr_file)\n",
    "                for word in hocr_df.iterrows():\n",
    "                    coords = word[1]['coords']\n",
    "                    (x1df,y1df,x2df,y2df) = coords\n",
    "                    box2 = [[x1df, y1df], [x2df, y1df], [x2df, y2df], [x1df, y2df]]\n",
    "                    words = word[1]['word']\n",
    "                    overlap_perc = calculate_iou(box1,box2)\n",
    "                    temp_dic = {}\n",
    "                    if overlap_perc > 0.80:\n",
    "                        if words != '-':\n",
    "                            word_list.append(words)\n",
    "                            bboxes_list.append(coords)\n",
    "                            label_id = label2id[label]                              \n",
    "                            ner_tags_list.append(label_id)\n",
    "                        \n",
    "                        custom_label_text['tokens'] = word_list\n",
    "                        custom_label_text['bboxes'] = bboxes_list\n",
    "                        custom_label_text['ner_tags'] = ner_tags_list\n",
    "\n",
    "    final_list.append(custom_label_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cce48850-6e63-4536-aebd-f3c5f569617f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'file_name': 'doc-0001.jpg',\n",
       "  'tokens': ['1428',\n",
       "   '4',\n",
       "   'aprycta',\n",
       "   '2022',\n",
       "   'r.',\n",
       "   'Vin',\n",
       "   'Opnos',\n",
       "   'Anexcangp',\n",
       "   'Eprenbesuy',\n",
       "   '19,Ten.8-910-704-09-21',\n",
       "   '711603577035'],\n",
       "  'bboxes': [[1058, 270, 1146, 302],\n",
       "   [1216, 271, 1238, 301],\n",
       "   [1252, 245, 1403, 315],\n",
       "   [1418, 270, 1507, 301],\n",
       "   [1522, 277, 1547, 301],\n",
       "   [775, 481, 823, 511],\n",
       "   [838, 480, 960, 520],\n",
       "   [972, 481, 1178, 520],\n",
       "   [1193, 481, 1410, 512],\n",
       "   [316, 1554, 750, 1591],\n",
       "   [374, 1601, 649, 1634]],\n",
       "  'ner_tags': [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 4]},\n",
       " {'id': 1,\n",
       "  'file_name': 'doc-0002.jpg',\n",
       "  'tokens': ['1428',\n",
       "   '04.08.2022',\n",
       "   'VngueugyanbHbiit',\n",
       "   'npeanpvHumatenb',\n",
       "   'Opnos',\n",
       "   'Anexcanap',\n",
       "   'Esrexbesniy',\n",
       "   '711603577035'],\n",
       "  'bboxes': [[1175, 962, 1282, 1000],\n",
       "   [1367, 962, 1617, 1000],\n",
       "   [461, 133, 790, 173],\n",
       "   [806, 139, 1149, 172],\n",
       "   [1163, 131, 1285, 171],\n",
       "   [1297, 131, 1503, 171],\n",
       "   [1517, 131, 1735, 162],\n",
       "   [357, 229, 631, 262]],\n",
       "  'ner_tags': [0, 1, 2, 2, 2, 2, 2, 4]},\n",
       " {'id': 2,\n",
       "  'file_name': 'doc-0003.jpg',\n",
       "  'tokens': ['iy',\n",
       "   'cKO',\n",
       "   'yn0«',\n",
       "   'A',\n",
       "   '4',\n",
       "   '214¥2',\n",
       "   '20227,',\n",
       "   '</.',\n",
       "   'Z.',\n",
       "   'dpiek',\n",
       "   'ae:',\n",
       "   'Cel',\n",
       "   '~f.',\n",
       "   'A.',\n",
       "   '‘3',\n",
       "   'F-',\n",
       "   'g',\n",
       "   'JG',\n",
       "   '06',\n",
       "   'DOS',\n",
       "   'KLE'],\n",
       "  'bboxes': [[1951, 321, 2090, 408],\n",
       "   [1151, 517, 1289, 572],\n",
       "   [1334, 541, 1456, 566],\n",
       "   [1479, 518, 1545, 569],\n",
       "   [1699, 500, 1795, 569],\n",
       "   [1799, 521, 1977, 580],\n",
       "   [2109, 515, 2279, 564],\n",
       "   [1180, 718, 1247, 775],\n",
       "   [1252, 717, 1308, 775],\n",
       "   [916, 676, 1108, 844],\n",
       "   [916, 725, 1209, 852],\n",
       "   [1214, 803, 1272, 849],\n",
       "   [1233, 863, 1285, 893],\n",
       "   [833, 874, 861, 899],\n",
       "   [863, 809, 929, 902],\n",
       "   [1211, 866, 1239, 897],\n",
       "   [1283, 859, 1317, 898],\n",
       "   [2386, 754, 2469, 802],\n",
       "   [2498, 751, 2569, 796],\n",
       "   [2638, 732, 2761, 810],\n",
       "   [2780, 749, 2891, 799]],\n",
       "  'ner_tags': [0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]},\n",
       " {'id': 3, 'file_name': 'doc-0004.jpg'},\n",
       " {'id': 4,\n",
       "  'file_name': 'doc-0005.jpg',\n",
       "  'tokens': ['Cxpar6un', 'A.A.', 'Posymeuko', '4.H.'],\n",
       "  'bboxes': [[307, 2716, 484, 2761],\n",
       "   [499, 2716, 582, 2752],\n",
       "   [341, 600, 565, 644],\n",
       "   [579, 598, 666, 641]],\n",
       "  'ner_tags': [2, 2, 2, 2]},\n",
       " {'id': 5,\n",
       "  'file_name': 'doc-0006.jpg',\n",
       "  'tokens': ['Ha', 'cbespalib', '2005'],\n",
       "  'bboxes': [[1358, 647, 1407, 672],\n",
       "   [1539, 634, 1732, 685],\n",
       "   [1963, 638, 2059, 672]],\n",
       "  'ner_tags': [1, 1, 1]}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8135830b-d45d-435a-a63a-96e4f882afcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97706427-95bc-4573-9107-fb4000955742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d0b32c28-09f8-494c-8d8f-ab74ff7ca984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(final_list, random_state=21, test_size=0.3)\n",
    "\n",
    "for detail  in final_list:\n",
    "    with open('final_list_text.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(str(detail))\n",
    "        f.write('\\n')\n",
    "        \n",
    "for detail  in train:\n",
    "    with open('train.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(str(detail))\n",
    "        f.write('\\n')\n",
    "        \n",
    "for detail  in test:\n",
    "    with open('test.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(str(detail))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862a3a6-316b-4f38-a742-a87002f02246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b7988-1c62-45f0-9eee-3b0d44127913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30f752-7144-44cb-9754-9767ba712894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "15a9dcbf-ea8c-4957-ba7e-2143ceaf05d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
      "     ---------------------------------------- 0.0/474.3 kB ? eta -:--:--\n",
      "     ----- --------------------------------- 61.4/474.3 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------- ----------------- 256.0/474.3 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 474.3/474.3 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Collecting huggingface-hub>=0.22.0\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "     ---------------------------------------- 0.0/436.4 kB ? eta -:--:--\n",
      "     ------------------------ ------------- 286.7/436.4 kB 8.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 436.4/436.4 kB 9.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.10.5-cp310-cp310-win_amd64.whl (379 kB)\n",
      "     ---------------------------------------- 0.0/379.2 kB ? eta -:--:--\n",
      "     ---------------------------------- -- 358.4/379.2 kB 11.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 379.2/379.2 kB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from datasets) (24.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "     ---------------------------------------- 0.0/177.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 177.6/177.6 kB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "     ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.6/25.1 MB 34.6 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.4/25.1 MB 30.7 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 5.6/25.1 MB 45.1 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 5.6/25.1 MB 45.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 6.8/25.1 MB 30.8 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 7.2/25.1 MB 28.7 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 7.7/25.1 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 8.6/25.1 MB 24.0 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 9.6/25.1 MB 23.6 MB/s eta 0:00:01\n",
      "     ---------------- ---------------------- 10.4/25.1 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------- --------------------- 11.4/25.1 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 12.3/25.1 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 14.0/25.1 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 14.0/25.1 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 14.0/25.1 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 14.0/25.1 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 14.1/25.1 MB 15.6 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 14.2/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 14.2/25.1 MB 13.6 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 14.3/25.1 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 14.6/25.1 MB 11.9 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 14.9/25.1 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 15.1/25.1 MB 10.9 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 15.4/25.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 15.6/25.1 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 15.8/25.1 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 15.9/25.1 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 16.1/25.1 MB 9.4 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 16.4/25.1 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 16.7/25.1 MB 8.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 16.9/25.1 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 17.6/25.1 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 18.1/25.1 MB 8.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 18.5/25.1 MB 8.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 19.0/25.1 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 19.6/25.1 MB 8.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 21.0/25.1 MB 8.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  24.8/25.1 MB 12.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  25.1/25.1 MB 14.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 25.1/25.1 MB 12.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.11.1-cp310-cp310-win_amd64.whl (110 kB)\n",
      "     ---------------------------------------- 0.0/110.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 110.2/110.2 kB ? eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aburilin\\projects\\elar\\notebook\\dai\\layoutlmv3\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, fsspec, frozenlist, filelock, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.0 dill-0.3.8 filelock-3.16.1 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.25.0 multidict-6.1.0 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0 yarl-1.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a85d4cc-7acb-4d8e-88d7-18aabc700288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "logger = datasets.logging.get_logger(__name__)\n",
    "_CITATION = \"\"\"\\\n",
    "@article{,\n",
    "  title={},\n",
    "  author={},\n",
    "  journal={},\n",
    "  year={},\n",
    "  volume={}\n",
    "}\n",
    "\"\"\"\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "This is a sample dataset for training layoutlmv3 model on custom annotated data.\n",
    "\"\"\"\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    return image, (w,h)\n",
    "\n",
    "def normalize_bbox(bbox, size):\n",
    "    return [\n",
    "        int(1000 * bbox[0] / size[0]),\n",
    "        int(1000 * bbox[1] / size[1]),\n",
    "        int(1000 * bbox[2] / size[0]),\n",
    "        int(1000 * bbox[3] / size[1]),\n",
    "    ]\n",
    "\n",
    "\n",
    "_URLS = []\n",
    "\n",
    "'''Edit your working directory folder path here if required. \n",
    "If this file is in the same folder as the \"layoutlmv3\" folder keep it as it is.\n",
    "'''\n",
    "data_path = r'./' \n",
    "\n",
    "class DatasetConfig(datasets.BuilderConfig):\n",
    "    \"\"\"BuilderConfig for InvoiceExtraction Dataset\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"BuilderConfig for InvoiceExtraction Dataset.\n",
    "        Args:\n",
    "          **kwargs: keyword arguments forwarded to super.\n",
    "        \"\"\"\n",
    "        super(DatasetConfig, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "class InvoiceExtraction(datasets.GeneratorBasedBuilder):\n",
    "    BUILDER_CONFIGS = [\n",
    "        DatasetConfig(name=\"InvoiceExtraction\", version=datasets.Version(\"1.0.0\"), description=\"InvoiceExtraction dataset\"),\n",
    "    ]\n",
    "\n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"id\": datasets.Value(\"string\"),\n",
    "                    \"tokens\": datasets.Sequence(datasets.Value(\"string\")),\n",
    "                    \"bboxes\": datasets.Sequence(datasets.Sequence(datasets.Value(\"int64\"))),\n",
    "                    \"ner_tags\": datasets.Sequence(\n",
    "                        datasets.features.ClassLabel(\n",
    "                            names = ['invoice_no', 'date', 'amount'] #Enter the list of labels that you have here.\n",
    "                            )\n",
    "                    ),\n",
    "                    \"image_path\": datasets.Value(\"string\"),\n",
    "                    \"image\": datasets.features.Image()\n",
    "                }\n",
    "            ),\n",
    "            supervised_keys=None,\n",
    "            citation=_CITATION,\n",
    "            homepage=\"\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        \"\"\"Returns SplitGenerators.\"\"\"\n",
    "        \"\"\"Uses local files located with data_dir\"\"\"\n",
    "        dest = os.path.join(data_path, 'layoutlmv3')\n",
    "\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": os.path.join(dest, \"train.txt\"), \"dest\": dest}\n",
    "            ),            \n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TEST, gen_kwargs={\"filepath\": os.path.join(dest, \"test.txt\"), \"dest\": dest}\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def _generate_examples(self, filepath, dest):\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dest, 'class_list.txt'), delimiter='\\s', header=None)\n",
    "        id2labels = dict(zip(df[0].tolist(), df[1].tolist()))\n",
    "\n",
    "\n",
    "        logger.info(\"⏳ Generating examples from = %s\", filepath)\n",
    "\n",
    "        item_list = []\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                item_list.append(line.rstrip('\\n\\r'))\n",
    "        print(item_list)\n",
    "        for guid, fname in enumerate(item_list):\n",
    "            print(fname)\n",
    "            data = ast.literal_eval(fname)\n",
    "            image_path = os.path.join(dest, data['file_name'])\n",
    "            image, size = load_image(image_path)\n",
    "            boxes = data['bboxes']\n",
    "\n",
    "            text = data['tokens']\n",
    "            label = data['ner_tags']\n",
    "            \n",
    "            #print(boxes)\n",
    "            #for i in boxes:\n",
    "            #  print(i)\n",
    "            boxes = [normalize_bbox(box, size) for box in boxes]\n",
    "            flag=0\n",
    "            #print(image_path)\n",
    "            for i in boxes:\n",
    "              #print(i)\n",
    "              for j in i:\n",
    "                if j>1000:\n",
    "                  flag+=1\n",
    "                  #print(j)\n",
    "                  pass\n",
    "            if flag>0: print(image_path)\n",
    " \n",
    "            yield guid, {\"id\": str(guid), \"tokens\": text, \"bboxes\": boxes, \"ner_tags\": label, \"image_path\": image_path, \"image\": image}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ab288-2282-4255-8a1f-c7c0b3401d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b6845-f11f-42c5-a076-a91e37da3935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aaeee4-9590-4864-a2d2-731c0d8b7ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
